{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sam.segment_anything.modeling import ImageEncoderViT\n",
    "from sam.segment_anything.modeling.common import LayerNorm2d\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from pprint import pprint \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDecoder(nn.Module):\n",
    "    def __init__(self, in_chans):\n",
    "        super().__init__()\n",
    "        activation = nn.GELU\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_chans, in_chans//2, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(in_chans//2),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(in_chans//2, in_chans//2//2, kernel_size=2, stride=2),\n",
    "            # LayerNorm2d(in_chans//2//2),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(in_chans//2//2, in_chans//2//2//2, kernel_size=2, stride=2),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(in_chans//2//2//2, 1, kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)  # BCHW\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TopoDecoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.hidden_dim = cfg.TOPO_DECODER.HIDDEN_DIM\n",
    "        self.num_heads = cfg.TOPO_DECODER.NUM_HEADS\n",
    "        self.depth = cfg.TOPO_DECODER.DEPTH\n",
    "        self.dim_ffn = cfg.TOPO_DECODER.DIM_FFN\n",
    "        self.ROI_size = cfg.TOPO_DECODER.ROI_SIZE\n",
    "        self.num_queries = cfg.TOPO_DECODER.NUM_QUERIES\n",
    "        \n",
    "        self.query_embed = nn.Embedding(self.num_queries, self.hidden_dim)\n",
    "        \n",
    "        # BUG 这里的rel_pos_embed实现不明确，或者说暂时不需要实现\n",
    "        # self.rel_pos_embed = nn.Parameter(torch.randn(size=(self.ROI_size, self.ROI_size)), requires_grad=True)\n",
    "        \n",
    "        # XXX 这里到底该用EncoderLayer还是DecoderLayer还有待商榷，亦或者先encoder再decoder？\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.hidden_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dim_feedforward=self.dim_ffn,\n",
    "            dropout=0.1,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.topo_decoder = nn.TransformerDecoder(decoder_layer, num_layers=self.depth)\n",
    "        \n",
    "        # TODO 看DETR如何输出固定数量的点（根据多个query输出多个点）\n",
    "        self.output_proj = nn.Linear(self.hidden_dim, 3)  # 输出[p, Δx, Δy]\n",
    "        \n",
    "\n",
    "    def _get_valid_rc(self, r, c, H, W):\n",
    "        '''由关键点的rc坐标输出对应有效范围的索引以及各自由此导致的需要padding的大小'''\n",
    "        ROI_size = self.cfg.TOPO_DECODER.ROI_SIZE\n",
    "        lst = [c-(ROI_size/2), c+(ROI_size/2), r-(ROI_size/2), r+(ROI_size/2)]\n",
    "        left, right, up, down = [int(x) for x in lst]\n",
    "        l_pad, r_pad, u_pad, d_pad = [0] * 4\n",
    "        if left < 0:\n",
    "            l_pad = -left\n",
    "            left = 0\n",
    "        if up < 0:\n",
    "            u_pad = -up\n",
    "            up = 0\n",
    "        if right > W:\n",
    "            r_pad = right - W\n",
    "            right = W\n",
    "        if down > H:\n",
    "            d_pad = down - H\n",
    "            down = H\n",
    "            \n",
    "        return (left, right, up, down), (l_pad, r_pad, u_pad, d_pad)\n",
    "    \n",
    "    \n",
    "    def crop_ROI_feature(self, upsampled_features, keypoints):\n",
    "        '''根据传入的多个keypoints截取相应的ROI_features'''\n",
    "        # upsampled_features: [B, 128, H, W]\n",
    "        # keypoints: [B, N_points, 2]\n",
    "        # 左和上取16，右和下取15 -> 32x32\n",
    "        B, C, H, W = upsampled_features.shape\n",
    "        batch_cropped_features = []\n",
    "        for sample in range(B):\n",
    "            sample_cropped_features= []\n",
    "            for point in keypoints[sample]:\n",
    "                r, c = point\n",
    "                (left, right, up, down), pad = self._get_valid_rc(r, c, H, W)\n",
    "                single_cropped_feature = upsampled_features[sample, :, up:down, left:right]\n",
    "                single_cropped_feature = F.pad(single_cropped_feature, pad=pad, mode='constant', value=0)\n",
    "                sample_cropped_features.append(single_cropped_feature)\n",
    "            sample_cropped_features = torch.stack(sample_cropped_features, dim=0)  # [N_points, C, H, W]\n",
    "            batch_cropped_features.append(sample_cropped_features)\n",
    "        batch_cropped_features = torch.stack(batch_cropped_features, dim=0) # [B, N_points, C, H, W]\n",
    "        \n",
    "        return batch_cropped_features\n",
    "    \n",
    "    \n",
    "    def forward(self, image_embeddings, upsampled_features, keypoints=None, keypoints_valid=None, with_asb_PE=False):\n",
    "        # image_embeddings: [B, 256, H, W]\n",
    "        # upsampled_features: [B, 128, H, W]\n",
    "        # keypoints: [B, N_points, 2]\n",
    "        B, C, H, W = image_embeddings.shape\n",
    "        image_embeddings = torch.detach(image_embeddings) \n",
    "        upsampled_features = torch.detach(upsampled_features)   # 使用detach, 因为对单个点的ROI的改动不应该反应到原始输入中，只是临时用用\n",
    "        if with_asb_PE:\n",
    "            # TODO 可能需要加入绝对位置编码\n",
    "            pass\n",
    "        # ROI_features = self.crop_ROI_feature(upsampled_features, keypoints) # [B, N_points, C, H, W]\n",
    "        # BUG 注意这里Batch中每个样本的N_points可能不一样，这就导致一个batch中最终会出现不同的输入序列长度 -> 那就处理成一样的\n",
    "        # TODO 解决每个batch的样本的N_points可能不一样的问题，否则就需要加入mask \n",
    "        N_points = self.cfg.TOPO_DECODER.NUM_POINTS\n",
    "        assert N_points == keypoints.shape[1], \"Input num_points not equal to pre-defined NUM_POINTS for per patch!\"   # 即便一个样本没有这么多个点也要提前pad好\n",
    "        ROI_features = upsampled_features[:, :, :, :].unsqueeze(1).repeat(1, N_points, 1, 1, 1)\n",
    "        # print(ROI_features.shape)\n",
    "        # TODO 还需要考量这里的valid如何才正确（对比samRoad）\n",
    "        keypoints_valid = keypoints_valid.view(B * N_points, H*W)\n",
    "        \n",
    "        \n",
    "        sum_C = C + ROI_features.shape[2]   # 256 + 128 = 384\n",
    "        # print(sum_C)\n",
    "        image_embeddings = image_embeddings.unsqueeze(1).repeat(1, N_points, 1, 1, 1)  #  -> [B, 1, C, H, W] -> [B, N_points, C, H, W] for concatenation\n",
    "        # print(image_embeddings.shape)\n",
    "        \n",
    "        # [B,N_points,C,H,W] -> [B*N_points,C,H*W] -> [B*N_points,H*W,C]\n",
    "        x = torch.concat([ROI_features, image_embeddings], dim=2).reshape(-1, sum_C, H*W).permute(0, 2, 1)\n",
    "        # print(x.shape)\n",
    "        # print(self.query_embed.weight.repeat(B, 1, 1).shape)\n",
    "        \n",
    "        # TODO 确定哪些位置是需要被mask掉的，也就是确定memory_key_padding_mask\n",
    "        x = self.topo_decoder(tgt=self.query_embed.weight.repeat(B*N_points, 1, 1), memory=x, memory_key_padding_mask=keypoints_valid)   # TODO 这里也许需要pos_embed\n",
    "        output_logits = self.output_proj(x) # num_queries个384维向量转成num_queries个3维向量\n",
    "        \n",
    "        return output_logits\n",
    "            \n",
    "            \n",
    "# 测试输出是否正常\n",
    "from utils import load_config\n",
    "\n",
    "cfg_path = './config/R2RC.yml'\n",
    "cfg = load_config(cfg_path)\n",
    "model = TopoDecoder(cfg)\n",
    "\n",
    "# image_embeddings = torch.rand(size=(4, 256, 32, 32))\n",
    "# upsampled_features = torch.rand(size=(4, 128, 32, 32))\n",
    "# y = model(image_embeddings, upsampled_features)\n",
    "# print(y.shape)\n",
    "\n",
    "# # TODO 测试一个batch内各个cropped的feat的后半程是否一样 -> yes!\n",
    "# upsampled_features = torch.rand(size=(4, 1, 512, 512))\n",
    "# image_embeddings = torch.rand(size=(4, 1, 32, 32))\n",
    "# upsampled_features[0] = torch.ones((1, 512, 512))\n",
    "# upsampled_features[1] = torch.ones((1, 512, 512)) + 1\n",
    "\n",
    "# image_embeddings[0] = torch.ones((1, 32, 32)) + 10\n",
    "# image_embeddings[1] = torch.ones((1, 32, 32)) + 20\n",
    "\n",
    "# keypoints = [[[100, 100], [200, 200]], \n",
    "#              [[100, 100], [200, 200]], \n",
    "#              [[100, 100], [200, 200]], \n",
    "#              [[100, 100], [200, 200]]]\n",
    "\n",
    "# batch_cropped_features = model.crop_ROI_feature(upsampled_features, keypoints)\n",
    "# print(batch_cropped_features.shape)\n",
    "# B, C, H, W = image_embeddings.shape\n",
    "# sum_C = C + batch_cropped_features.shape[2]   # 256 + 128 = 384\n",
    "# b, n_points, c, h, w = batch_cropped_features.shape\n",
    "# image_embeddings = image_embeddings.unsqueeze(1).repeat(1, n_points, 1, 1, 1)   # -> [B, 1, C, H, W]\n",
    "# print(image_embeddings.shape)\n",
    "# x = torch.concat([batch_cropped_features, image_embeddings], dim=2)\n",
    "# # x = torch.concat([batch_cropped_features, image_embeddings], dim=2).reshape(-1, sum_C, H*W).permute(0, 2, 1)\n",
    "# print(x.shape)\n",
    "\n",
    "# print(x[1, 0, 0])\n",
    "# print(x[1, 0, 1])\n",
    "# print(x[1, 1, 0])\n",
    "# print(x[1, 1, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4230, 0.6379, 0.0859, 0.4980],\n",
      "        [0.8984, 0.1095, 0.3243, 0.9580],\n",
      "        [0.2919, 0.7649, 0.9141, 0.8269],\n",
      "        [0.4134, 0.0099, 0.0915, 0.6846]])\n",
      "torch.Size([3, 3])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4230, 0.6379, 0.0859],\n",
      "        [0.0000, 0.8984, 0.1095, 0.3243],\n",
      "        [0.0000, 0.2919, 0.7649, 0.9141]])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=5000, edgeitems=3, linewidth=100)\n",
    "r, c = 1, 1\n",
    "cfg.TOPO_DECODER.ROI_SIZE = 4\n",
    "a = TopoDecoder(cfg)\n",
    "(left, right, up, down), pad = a._get_valid_rc(r, c, 4, 4)\n",
    "feat = torch.rand((4, 4))\n",
    "print(feat)\n",
    "cropped_feat = feat[up:down, left:right]\n",
    "print(cropped_feat.shape)\n",
    "print(F.pad(cropped_feat, pad))\n",
    "print(F.pad(cropped_feat, pad).shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureUpsampler(nn.Module):\n",
    "    def __init__(self, cfg, in_chans=256):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # NOTE 或许可以改成可学习式的上采样器，但考虑到参数激增问题先实现一个插值的就好\n",
    "        # activation = nn.GELU\n",
    "        # self.upsampler = nn.Sequential(\n",
    "        #     nn.ConvTranspose2d(in_chans, in_chans//2, kernel_size=2, stride=2),\n",
    "        #     LayerNorm2d(in_chans//2),\n",
    "        #     activation(),\n",
    "        #     nn.ConvTranspose2d(in_chans//2, in_chans//2//2, kernel_size=2, stride=2),\n",
    "        #     LayerNorm2d(in_chans//2//2),\n",
    "        #     activation(),\n",
    "        #     nn.ConvTranspose2d(in_chans//2//2, in_chans//2//2//2, kernel_size=2, stride=2),\n",
    "        #     activation(),\n",
    "        #     nn.ConvTranspose2d(in_chans//2//2//2, 1, kernel_size=2, stride=2)\n",
    "        # )\n",
    "        \n",
    "        \n",
    "    def forward(self, image_embeddings):\n",
    "        '''该函数将image_embedding直接上采样到原尺寸的特征图'''\n",
    "        # image_embedding: [B, C, H, W]\n",
    "        H, W = self.cfg.PATCH_SIZE, self.cfg.PATCH_SIZE\n",
    "        return F.interpolate(image_embeddings, size=(W, W), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class R2RC(pl.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # overall cfg\n",
    "        self.cfg = cfg\n",
    "        self.image_size = cfg.PATCH_SIZE\n",
    "        \n",
    "        # data cfg\n",
    "        self.register_buffer('mean', torch.Tensor(cfg.MEAN).view(-1, 1, 1), persistent=False) \n",
    "        self.register_buffer('std', torch.Tensor(cfg.STD).view(-1, 1, 1), persistent=False)\n",
    "        \n",
    "        # model cfg\n",
    "        self.encoder_output_dim = cfg.ENCODER.ENCODER_OUTPUT_DIM\n",
    "        self.vit_patch_size = cfg.ENCODER.VIT_PATCH_SIZE\n",
    "        \n",
    "        assert cfg.ENCODER.BACKBONE in ['SAM-vit-b', ...], f\"{cfg.ENCODER.BACKBONE} is not a valid backbone! \"\n",
    "        if cfg.ENCODER.BACKBONE == 'SAM-vit-b':\n",
    "            self.encoder_embed_dim = 768\n",
    "            self.encoder_num_transformer_blocks = 12\n",
    "            self.encoder_num_heads = 12\n",
    "            self.encoder_global_attn_indexes = [2, 5, 8, 11]\n",
    "        \n",
    "        self.image_encoder = self._init_image_encoder()\n",
    "        self.mask_decoder = self._init_mask_decoder()\n",
    "        self.feature_upsampler = self._init_feature_upsampler()\n",
    "        self.topo_decoder = self._init_topo_decoder()\n",
    "        \n",
    "        \n",
    "        # TODO criterion\n",
    "        \n",
    "        # TODO metrics\n",
    "        \n",
    "        \n",
    "        self._load_pretrained_weights()\n",
    "        \n",
    "        \n",
    "    def _init_image_encoder(self):\n",
    "        return ImageEncoderViT(\n",
    "            img_size=self.image_size,\n",
    "            patch_size=self.vit_patch_size,\n",
    "            in_chans=3,\n",
    "            embed_dim=self.encoder_embed_dim,\n",
    "            depth=self.encoder_num_transformer_blocks,\n",
    "            num_heads=self.encoder_num_heads,\n",
    "            mlp_ratio=4.0,\n",
    "            out_chans=self.encoder_output_dim,\n",
    "            qkv_bias=True,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "            act_layer=nn.GELU,\n",
    "            use_abs_pos=True,\n",
    "            use_rel_pos=True,\n",
    "            window_size=14,\n",
    "            global_attn_indexes=self.encoder_global_attn_indexes\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _init_mask_decoder(self):\n",
    "        return MaskDecoder(self.encoder_output_dim)\n",
    "    \n",
    "    \n",
    "    def _init_topo_decoder(self):\n",
    "        return TopoDecoder(self.cfg)\n",
    "    \n",
    "    def _init_feature_upsampler(self):\n",
    "        return FeatureUpsampler(self.cfg)\n",
    "    \n",
    "    \n",
    "    def _resize_sam_pos_embedding(self, pretrained_state_dict):\n",
    "        '''把SAM与训练权重里面的abs_pos_embed和rel_pos_embed都resize一下'''\n",
    "        new_state_dict = {k:v for k, v in pretrained_state_dict.items()}\n",
    "        pos_embed = new_state_dict['image_encoder.pos_embed']   # BHWC\n",
    "        token_size = int(self.image_size // self.vit_patch_size)\n",
    "        if pos_embed.shape[1] != token_size:    # != 1024/16\n",
    "            # abs pos\n",
    "            pos_embed = pos_embed.permute(0, 3, 1, 2)   # ->BCHW for interpolate\n",
    "            pos_embed = F.interpolate(pos_embed, size=(token_size, token_size), mode='bilinear', align_corners=False)\n",
    "            new_state_dict['image_encoder.pos_embed'] = pos_embed.permute(0, 2, 3, 1)\n",
    "            # rel_pos\n",
    "            rel_pos_key_pattern = '{}.attn.rel_pos'\n",
    "            global_rel_pos_keys = []\n",
    "            for idx in self.encoder_global_attn_indexes:\n",
    "                for k in new_state_dict.keys():\n",
    "                    if rel_pos_key_pattern.format(idx) in k:\n",
    "                        global_rel_pos_keys.append(k)\n",
    "            for k in global_rel_pos_keys:\n",
    "                rel_pos_embed = new_state_dict[k]\n",
    "                # XXX 把序列长度和通道数看成是HW矩阵以进行空间插值，这样做是否合理？是不是应该重新训练这一部分？\n",
    "                H, W = rel_pos_embed.shape  # W 其实是对应的通道数\n",
    "                rel_pos_embed = rel_pos_embed.unsqueeze(0).unsqueeze(0) # HW -> BCHW [1,1,H,W]\n",
    "                rel_pos_embed = F.interpolate(rel_pos_embed, size=(2*token_size - 1, W), mode='bilinear', align_corners=False)\n",
    "                new_state_dict[k] = rel_pos_embed[0, 0, :, :]\n",
    "        return new_state_dict\n",
    "    \n",
    "    \n",
    "    def _load_pretrained_weights(self):\n",
    "        with open(self.cfg.ENCODER.SAM_CKPT_PATH, 'rb') as f:\n",
    "            state_dict = torch.load(f)\n",
    "            state_dict = self._resize_sam_pos_embedding(state_dict)\n",
    "            \n",
    "        new_state_dict = {}\n",
    "        matched_names = []\n",
    "        unmatched_names = []\n",
    "        for n, p in self.named_parameters():   # name, param\n",
    "            if n in state_dict and p.shape==state_dict[n].shape:\n",
    "                new_state_dict[n] = state_dict[n]\n",
    "                matched_names.append(n)\n",
    "            else:\n",
    "                unmatched_names.append(n)\n",
    "        \n",
    "        if self.cfg.dev_run:\n",
    "            pprint(\"========== Matched names ==========\")\n",
    "            pprint(matched_names)\n",
    "            print()\n",
    "            pprint(\"xxxxxxxxxx Unmatched names xxxxxxxxxx\")\n",
    "            pprint(unmatched_names)\n",
    "            \n",
    "        self.load_state_dict(new_state_dict, strict=False)\n",
    "            \n",
    "        \n",
    "    def forward(self, rgb, keypoints):\n",
    "        # rgb: [B, H, W, C]\n",
    "        # keypoints: [B, N_points, 2]\n",
    "        x = rgb.permute(0, 3, 1, 2) # [B, C, H, W]\n",
    "        x = (x - self.mean) / self.std\n",
    "        \n",
    "        image_embeddings = self.image_encoder(x)\n",
    "        kpt_mask_logits = self.mask_decoder(image_embeddings)\n",
    "        upsampled_features = self.feature_upsampler(image_embeddings)\n",
    "        \n",
    "        # TODO cropped_upsampled_features或许应该加上相对位置编码\n",
    "        # TODO topo_decoder 还应该接受多个queries用以输出多个候选点\n",
    "        # pred_next_nodes: [B, N, 3]\n",
    "        # [\n",
    "        #   [p1, Δx1, Δy1],\n",
    "        #   [p2, Δx2, Δy2],\n",
    "        #       ...\n",
    "        #   [p6, Δx6, Δy6]\n",
    "        # ]\n",
    "        # BUG 注意由于预测的点是无序的，所以在计算损失时需要匹配一下\n",
    "        pred_next_nodes_logits = self.topo_decoder(upsampled_features, image_embeddings, keypoints)\n",
    "        \n",
    "        return kpt_mask_logits, pred_next_nodes_logits\n",
    "    \n",
    "    \n",
    "    def training_setp(self, batch, batch_idx):\n",
    "        loss = None\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def cfgure_optimizers(self):\n",
    "        optimizer = None\n",
    "        lr_scheduler = None\n",
    "        \n",
    "        return  {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75395/3878698429.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 32, 32])\n",
      "torch.Size([4, 256, 512, 512])\n",
      "torch.Size([4, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from utils import load_config\n",
    "\n",
    "cfg_path = './config/R2RC.yml'\n",
    "cfg = load_config(cfg_path)\n",
    "cfg.dev_run = False\n",
    "\n",
    "\n",
    "model = R2RC(cfg=cfg)\n",
    "\n",
    "rgb = torch.rand(size=(4, 3, 512, 512))\n",
    "image_embeddings = model.image_encoder(rgb)\n",
    "upsampler_features = model.feature_upsampler(image_embeddings)\n",
    "mask = model.mask_decoder(image_embeddings)\n",
    "print(image_embeddings.shape)\n",
    "print(upsampler_features.shape)\n",
    "print(mask.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R2RC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
